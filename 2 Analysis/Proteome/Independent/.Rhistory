# Plot histograms
colnames(WT_hits)[5] = c("RA")
colnames(CKO_hits)[5] = c("RA")
axis_text_size = 12
axis_label_size = 12
histogram <- function(data) {
ggplot(data, aes(x = log10(RA))) +
geom_histogram(binwidth = 0.1, col = "white") +
xlim(-1.6, 0.2) +
theme( # make it look nice
panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"),
axis.text=element_text(colour="black", size = axis_text_size),
axis.title = element_text(colour = "black", size = axis_label_size)) +
labs(title = deparse(substitute(data))) +
ylab("Count")
}
p1 <- histogram(WT_hits)
p2 <- histogram(CKO_hits)
ggsave(plot = grid.arrange(p1, p2, nrow = 2),
"RA histograms.pdf",
width = 5, height = 8)
grid.arrange(p1, p2, nrow = 2)
# Use density plots to visualise WT + CKO together (for sanity - these are not good for presentation because the distributions are so close together)
desnsity_plots <- cbind(WT_hits$Log10RA, CKO_hits$Log10RA)
colnames(desnsity_plots) <- c("WT", "CKO")
desnsity_plots <- melt(desnsity_plots)
desnsity_plots <- desnsity_plots[, -1]
colnames(desnsity_plots) <- c("Genotype", "Log10RA")
WT_plot <- ggplot(desnsity_plots, aes(x = Log10RA, fill = Genotype)) +
geom_density(alpha = 0.5) +
scale_fill_manual(values= c("grey", "red")) +
labs(title = "Relative amplitude distribution", x = expression("Log"[10]*"(Relative amplitude)"), y = "Probability density") +
theme(
# Remove panel border
panel.border = element_blank(),
# Remove panel grid lines
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
# Remove panel background
panel.background = element_blank(),
# Add axis line
axis.line = element_line(colour = "black"),
axis.text=element_text(colour="black")
)
WT_plot
ggsave(WT_plot, file="Density plots relative amplitude.pdf", width=4, height=3)
# How many CKO rhythmic proteins are rhythmic in WT but below the 10% RA threshold?
common_genes <- intersect(CKO_hits$Gene_names, WT_hits$Gene_names)
common_list <- cbind(common_genes)
common_list <- as.data.frame(common_list)
common_list <- as.data.frame(common_list[!is.na(common_list)])
colnames(common_list) <- c("common_genes")
# Select pVal + relative amplitude for the intersecting proteins
for (i in 1:nrow(common_list)) {
common_list$WT_pVal[i] <- WT_hits[which(WT_hits$Gene_names %in% common_list$common_genes[i]), 4]
common_list$CKO_pVal[i] <- CKO_hits[which(CKO_hits$Gene_names %in% common_list$common_genes[i]), 4]
common_list$WT_RA[i] <- WT_hits[which(WT_hits$Gene_names %in% common_list$common_genes[i]), 5]
common_list$CKO_RA[i] <- CKO_hits[which(CKO_hits$Gene_names %in% common_list$common_genes[i]), 5]
}
colnames(common_list) <- c("Gene name", "WT pVal", "CKO pVal", "WT Relative Amplitude", "CKO Relative Amplitude")
c <- common_list
c$`Protein name` <- WT_hits[which(WT_hits$Gene_names %in% c$`Gene name`) ,2]
c$uniprot <- WT_hits[which(WT_hits$Gene_names %in% c$`Gene name`) ,1]
c$amplification <- c$`CKO Relative Amplitude` / c$`WT Relative Amplitude`
c$logamp <- log10(c$amplification)
write.csv(c, "CKO_all_amplified_proteins.csv", row.names = F)
# Setup
# Clear your workspace
rm(list=ls())
# Load useful packages
if (!require("tidyverse")) {
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
}
if (!require("BiocManager")) {
install.packages("BiocManager", dependencies = TRUE)
}
if (!require("edgeR")) {
BiocManager::install("edgeR")
library(edgeR)
}
if (!require("psych")) {
install.packages("psych", dependencies = TRUE)
library(psych)
}
# Make a list of all the .txt files in your working directory just so you can see what's going on
X = list.files(".",".txt")
# read the raw data file, and ensure "NaN" is interpreted as NA
data_start <- read.delim("CRM3962_6set_MQ1633C_phos_noFil_m8b.txt", na = c("NaN"), stringsAsFactors = F)
data_start <- data_start[-1,] # Remove the first row which is just junk
data_start[, 1:60] <- sapply(data_start[,1:60], as.numeric) # Make the data numeric
missing_data <- apply(cbind(data_start[,13], data_start[,15]), 1, mean) # Infer missing values for CKO sample 12
data_start <- cbind(data_start[1:13], missing_data, data_start[15:ncol(data_start)])
# Remove rows which contain NaN within the data columns
data_start <- data_start[rowSums(is.na(data_start[, 1:60])) == 0,]
colnames(data_start)
data_start$Phosphosite <- paste(data_start$Protein, "_", data_start$Amino.acid, data_start$Position, data_start$Multiplicity, sep = "")
# Extract data only
WT_data_raw <- as.data.frame(data_start[31:60])
CKO_data_raw <- as.data.frame(data_start[1:30])
#See what raw data looks like
boxplot(log2(WT_data_raw), col = rep(c("red", "green", "blue"), each = 10),
main = "WT Raw data: \nExp1 (red), Exp2 (green), Exp3 (blue)",
xlab = 'TMT Sample', ylab = 'log2 of Intensity', notch = TRUE)
boxplot(log2(CKO_data_raw), col = rep(c("red", "green", "blue"), each = 10),
main = "CKO Raw data: \nExp1 (red), Exp2 (green), Exp3 (blue)",
xlab = 'TMT Sample', ylab = 'log2 of Intensity', notch = TRUE)
# can also look at density plots (like a distribution histogram)
plotDensities(log2(WT_data_raw), col = rep(c('red', 'green', 'blue'), 6),
main = 'WT Raw data')
plotDensities(log2(CKO_data_raw), col = rep(c('red', 'green', 'blue'), 6),
main = 'CKO Raw data')
# check the column totals (per channel sums)
format(round(colSums(WT_data_raw), digits = 0), big.mark = ",")
format(round(colSums(CKO_data_raw), digits = 0), big.mark = ",")
#Sample loading normalisation + IRS Normalisation (Phil Wilmarth, OHSU PSR Core, January 2018)
# separate the TMT data by experiment
exp1_raw <- WT_data_raw[c(1:10)]
exp2_raw <- WT_data_raw[c(11:20)]
exp3_raw <- WT_data_raw[c(21:30)]
exp4_raw <- CKO_data_raw[c(1:10)]
exp5_raw <- CKO_data_raw[c(11:20)]
exp6_raw <- CKO_data_raw[c(21:30)]
# figure out the global scaling value
target <- mean(c(colSums(exp1_raw), colSums(exp2_raw), colSums(exp3_raw), colSums(exp4_raw), colSums(exp5_raw), colSums(exp6_raw)))
# do the sample loading normalization before the IRS normalization
# there is a different correction factor for each column
norm_facs <- target / colSums(exp1_raw)
exp1_sl <- sweep(exp1_raw, 2, norm_facs, FUN = "*")
norm_facs <- target / colSums(exp2_raw)
exp2_sl <- sweep(exp2_raw, 2, norm_facs, FUN = "*")
norm_facs <- target / colSums(exp3_raw)
exp3_sl <- sweep(exp3_raw, 2, norm_facs, FUN = "*")
norm_facs <- target / colSums(exp4_raw)
exp4_sl <- sweep(exp4_raw, 2, norm_facs, FUN = "*")
norm_facs <- target / colSums(exp5_raw)
exp5_sl <- sweep(exp5_raw, 2, norm_facs, FUN = "*")
norm_facs <- target / colSums(exp6_raw)
exp6_sl <- sweep(exp6_raw, 2, norm_facs, FUN = "*")
# make a pre-IRS data frame after sample loading norms
data_sl <- cbind(exp1_sl, exp2_sl, exp3_sl, exp4_sl, exp5_sl, exp6_sl)
# see what the SL normalised data look like
boxplot(log2(data_sl), col = rep(c("red", "green", "blue"), each = 10),
main = "Sample loading (SL) normalized data: \nExp1 (red), Exp2 (green), Exp3 (blue)",
xlab = 'TMT Sample', ylab = 'log2 of Intensity', notch = TRUE)
# can also look at density plots (like a distribution histogram)
plotDensities(log2(data_sl), col = rep(c("red", "green", "blue"), 10), main = "SL data")
# check the columnn totals
format(round(colSums(data_sl), digits = 0), big.mark = ",")
# make working frame with row sums from each frame
irs <- tibble(apply(exp1_sl[,9:10], 1, mean), apply(exp2_sl[,9:10], 1, mean), apply(exp3_sl[,9:10], 1, mean), apply(exp4_sl[,9:10], 1, mean), apply(exp5_sl[,9:10], 1, mean), apply(exp6_sl[,9:10], 1, mean))
colnames(irs) <- c("mean1", "mean2", "mean3", "mean4", "mean5", "mean6")
# get the geometric average intensity for each protein
irs$average <- apply(irs, 1, mean)
# compute the scaling factor vectors
irs$fac1 <- irs$average / irs$mean1
irs$fac2 <- irs$average / irs$mean2
irs$fac3 <- irs$average / irs$mean3
irs$fac4 <- irs$average / irs$mean4
irs$fac5 <- irs$average / irs$mean5
irs$fac6 <- irs$average / irs$mean6
# make new data frame with normalized data
data_irs <- exp1_sl * irs$fac1
data_irs <- cbind(data_irs, exp2_sl * irs$fac2)
data_irs <- cbind(data_irs, exp3_sl * irs$fac3)
data_irs <- cbind(data_irs, exp4_sl * irs$fac4)
data_irs <- cbind(data_irs, exp5_sl * irs$fac5)
data_irs <- cbind(data_irs, exp6_sl * irs$fac6)
# see what the IRS data look like
boxplot(log2(data_irs), col = rep(c("red", "green", "blue"), each = 10),
main = "Internal Reference Scaling (IRS) normalized data: \nExp1 (red), Exp2 (green), Exp3 (blue)",
xlab = 'TMT Sample', ylab = 'log2 of Intensity', notch = TRUE)
# can also look at density plots (like a distribution histogram)
plotDensities(log2(data_irs), col = rep(c("red", "green", "blue"), 10), main = "IRS data")
# check column totals
format(round(colSums(data_irs), digits = 0), big.mark = ",")
# Normalise the IRS data according to total protein levels
# Load the list of total proteome and normalised data
total_proteome <- read.csv("All_normalised_data.csv", stringsAsFactors = F)
# Select only proteins that are present in the total proteome
data_irs <- data_irs %>% select(-contains("P")) # Remove the pools from the dataframe
# Normalise the IRS data according to total protein levels
# Load the list of total proteome and normalised data
total_proteome <- read.csv("All_normalised_data.csv", stringsAsFactors = F)
# Select only proteins that are present in the total proteome
data_irs <- data_irs %>% dplyr::select(-contains("P")) # Remove the pools from the dataframe
data_irs_adj <- cbind(data_irs, data_start[,61:ncol(data_start)])
data_irs_adj <- data_irs_adj[which(data_irs_adj$Protein %in% total_proteome$uniprot),]
# Normalise all values according to the protein abundance (from total proteome analysis)
total_proteome_max <- as.numeric(apply(total_proteome, 1, function(x) max(x[4:ncol(total_proteome)])))
# Initialise matrix
total_proteome_normalised <- matrix(0, nrow(total_proteome), 48)
# Divide protein intensities by their maximum
for (i in 1:nrow(total_proteome)) {
total_proteome_normalised[i,] <- t(apply(total_proteome[i, 4:ncol(total_proteome)], 1, function(x) x / total_proteome_max[i] ))
}
# Make it a data frame and attach protein + gene names
total_proteome_normalised <- as.data.frame(total_proteome_normalised)
total_proteome_normalised <- cbind(total_proteome_normalised, total_proteome[,1:3])
# Normalise the phospho data
phospho_normalised <- as.data.frame(matrix(0, nrow = nrow(data_irs_adj), 48))
for (i in 1:nrow(data_irs_adj)) {
phospho_normalised[i,] <- unlist(apply(data_irs_adj[i, 1:48], 1,
function(x) x / total_proteome_normalised[which(total_proteome_normalised$uniprot %in% data_irs_adj$Protein[i]),1:48]))
}
phospho_normalised <- cbind(phospho_normalised, data_irs_adj[,49:ncol(data_irs_adj)])
colnames(phospho_normalised) <- colnames(data_irs_adj)
# see what the resultant data look like
boxplot(log2(phospho_normalised[,1:48]), col = rep(c("red", "green", "blue"), each = 8),
main = "Internal Reference Scaling (IRS) normalized data: \nExp1 (red), Exp2 (green), Exp3 (blue)",
xlab = 'TMT Sample', ylab = 'log2 of Intensity', notch = TRUE)
# can also look at density plots (like a distribution histogram)
plotDensities(log2(phospho_normalised[,1:48]), col = rep(c("red", "green", "blue"), 8), main = "IRS data")
# check column totals
format(round(colSums(phospho_normalised[,1:48]), digits = 0), big.mark = ",")
# see how things cluster now that we have nice boxplots and density plots
plotMDS(log2(phospho_normalised[,1:48]), col = rep(c("red", "green", "blue"), each = 8),
main = "SL/TMM clusters group by TMT experiment")
# Reorganise the dataframe to export a nice spreadsheet
master_list <- cbind(phospho_normalised[49:ncol(phospho_normalised)], phospho_normalised[1:48])
write.csv(master_list, "All_phospho_normalised_data.csv", row.names = F)
# Setup
# Clear your workspace
rm(list=ls())
# Load useful packages
if (!require("tidyverse")) {
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
}
if (!require("BiocManager")) {
install.packages("BiocManager", dependencies = TRUE)
}
if (!require("rain")) {
BiocManager::install("rain")
library(rain)
}
# Make a list of all the .txt files in your working directory just so you can see what's going on
X = list.files(".",".csv")
# read the raw data file
data_start <- read.csv("All_phospho_normalised_data.csv", stringsAsFactors=FALSE)
# re-create dataframe with just normalised data - delete first 3 columns
data_irs <- data_start[,-1:-24]
colnames(data_start)
# Create preparation matrices for RAIN input
WT_RAIN_prep <- cbind(data_irs[1:24])
CKO_RAIN_prep <- cbind(data_irs[25:48])
#Transpose to create the inputs for RAIN.
WT_RAIN_input <- t(WT_RAIN_prep)
CKO_RAIN_input <- t(CKO_RAIN_prep)
#create matrix for RAIN output
WT_RAIN_output <- as.data.frame(matrix(0,nrow(WT_RAIN_prep), 4))
CKO_RAIN_output <- as.data.frame(matrix(0,nrow(CKO_RAIN_prep), 4))
#Perform RAIN analysis
step = 3 #number of hours between timepoints
period = 24 #specify period length to test for
rep = 1 #number of biological replicates
# Perform the RAIN analysis here
# RAIN automatically provides p values corrected for multiple testing using the adaptive Benjamini-Hochberg method
# See the paper for more details: Thaben and Westermark, 2014, JBR. https://journals.sagepub.com/doi/10.1177/0748730414553029
WT_RAIN_output <- rain(WT_RAIN_input, step, period, nr.series = rep)
CKO_RAIN_output <- rain(CKO_RAIN_input, step, period, nr.series = rep)
#Assign gene names to the rows
names <- cbind(data_start$Phosphosite, data_start$Protein, data_start$Protein.names, data_start$Gene.names)
WT_RAIN_output_1 <- cbind(names, WT_RAIN_output)
CKO_RAIN_output_1 <- cbind(names, CKO_RAIN_output)
colnames(WT_RAIN_output_1) <- c("Phosphosite", "Uniprot_ID",  "Protein_names", "Gene_names", "pVal", "phase", "peak.shape", "period")
colnames(CKO_RAIN_output_1) <- c("Phosphosite", "Uniprot_ID",  "Protein_names", "Gene_names", "pVal", "phase", "peak.shape", "period")
#Export the significant + insignificant hits including normalised data + also export total data
WT_results<- cbind(WT_RAIN_output_1, data_irs[1:24])
CKO_results <- cbind(CKO_RAIN_output_1, data_irs[25:48])
write.csv(WT_results[WT_results$pVal <= 0.05,], "WT_phosphoproteome_all_significant_RAIN.csv", row.names = F)
write.csv(CKO_results[CKO_results$pVal <= 0.05,], "CKO_phosphoproteome_all_significant_RAIN.csv", row.names = F)
write.csv(WT_results[WT_results$pVal > 0.05,], "WT_phosphoproteome_all_insignificant_RAIN.csv", row.names = F)
write.csv(CKO_results[CKO_results$pVal > 0.05,], "CKO_phosphoproteome_all_insignificant_RAIN.csv", row.names = F)
write.csv(WT_results, "WT.p_RAIN_results.csv", row.names = F)
write.csv(CKO_results, "CKO.p_RAIN_results.csv", row.names = F)
# Setup
# Clear your workspace
rm(list=ls())
# Load useful packages
if (!require("tidyverse")) {
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
}
if (!require("plotly")) {
install.packages("plotly", dependencies = TRUE)
library(plotly)
}
library(gridExtra)
#make a list of all the .csv files in your working directory just so you can see what's going on
X = list.files(".",".csv")
# read the raw data files
WT_data <- read.csv("WT_phosphoproteome_all_significant_RAIN.csv")
WT_data_start <- WT_data[,-1:-8]
WT_data_start <- t(WT_data_start)
colnames(WT_data_start) <- WT_data$Uniprot_ID
CKO_data <- read.csv("CKO_phosphoproteome_all_significant_RAIN.csv")
CKO_data_start <- CKO_data[,-1:-8]
CKO_data_start <- t(CKO_data_start)
colnames(CKO_data_start) <- CKO_data$Uniprot_ID
#Detrend the data using a 24h moving average
#Set up useful parameters for detrending
period = 24     #set period length to detrend
n_samples = ncol(WT_data_start)   #total number of samples
time <- seq(24,93,3)   #list of time-points
row_days = round(period/(time[2] - time [1]))    #number of rows that correspond to 24 hours (or 1 period length)
WT_detrended_data <- matrix(nrow=length(WT_data_start[,1]) - (row_days), ncol=ncol(WT_data_start))  #initialise a new matrix for the detrended data, excluding the first and last 12 hours (i.e. half of period)
colnames(WT_detrended_data) <- colnames(WT_data_start)     #set up the column names
#Fill up the matrix with the detrended data
for (i in 1:n_samples){
for (j in 1:length(WT_detrended_data[,1])){
WT_detrended_data[j,i] = WT_data_start[row_days/2+j,i] - mean(WT_data_start[j:(j + row_days),i])
}
}
#Export the detrended data as a .csv file
write.csv(WT_detrended_data, file = "WT.p_Detrended_data.csv", row.names = F)
# SAME FOR CKO
period = 24     #set period length to detrend
n_samples = ncol(CKO_data_start)   #total number of samples
time <- seq(24,93,3)   #list of time-points
row_days = round(period/(time[2] - time [1]))    #number of rows that correspond to 24 hours (or 1 period length)
CKO_detrended_data <- matrix(nrow=length(CKO_data_start[,1]) - (row_days), ncol=ncol(CKO_data_start))  #initialise a new matrix for the detrended data, excluding the first and last 12 hours (i.e. half of period)
colnames(CKO_detrended_data) <- colnames(CKO_data_start)     #set up the column names
#Fill up the matrix with the detrended data
for (i in 1:n_samples){
for (j in 1:length(CKO_detrended_data[,1])){
CKO_detrended_data[j,i] = CKO_data_start[row_days/2+j,i] - mean(CKO_data_start[j:(j + row_days),i])
}
}
#Export the detrended data as a .csv file
write.csv(CKO_detrended_data, file = "CKO.p_Detrended_data.csv", row.names = F)
#Calculate relative amplitude
#Calculate Means of each column
WT_Baselines <- as_tibble(colMeans(WT_data_start))
WT_Baselines <- rename(WT_Baselines, Means = value)
#Calculate peak-trough amplitude from detrended data
WT_Amplitudes <- as_tibble(apply(WT_detrended_data, 2, function(df) {max(df)-min(df)}))
WT_Relative_amplitudes <- as_tibble(WT_Amplitudes/WT_Baselines)
WT_Relative_amplitudes <- rename(WT_Relative_amplitudes, WT_Relative_amplitudes = value)
#Calculate Means of each column
CKO_Baselines <- as_tibble(colMeans(CKO_data_start))
CKO_Baselines <- rename(CKO_Baselines, Means = value)
#Calculate peak-trough amplitude from detrended data
CKO_Amplitudes <- as_tibble(apply(CKO_detrended_data, 2, function(df) {max(df)-min(df)}))
CKO_Relative_amplitudes <- as_tibble(CKO_Amplitudes/CKO_Baselines)
CKO_Relative_amplitudes <- rename(CKO_Relative_amplitudes, CKO_Relative_amplitudes = value)
#Export files
WT_total_data <- cbind(WT_data[1:8], WT_Relative_amplitudes, WT_data[9:ncol(WT_data)])
WT_big_hits <- WT_total_data[WT_total_data$WT_Relative_amplitudes >= 0.1,]
write.csv(WT_total_data, "WT.p_all_hits.csv", row.names = F)
CKO_total_data <- cbind(CKO_data[1:8], CKO_Relative_amplitudes, CKO_data[9:ncol(CKO_data)])
CKO_big_hits <- CKO_total_data[CKO_total_data$CKO_Relative_amplitudes >= 0.1,]
write.csv(CKO_total_data, "CKO.p_all_hits.csv", row.names = F)
# Export files with baselines
WT_export <- cbind(WT_data[1:8], WT_Relative_amplitudes, WT_Baselines, WT_data[9:ncol(WT_data)])
CKO_export <- cbind(CKO_data[1:8], CKO_Relative_amplitudes, CKO_Baselines, CKO_data[9:ncol(WT_data)])
write.csv(WT_export, "WT.p_RA_B.csv", row.names = F)
write.csv(CKO_export, "CKO.p_RA_B.csv", row.names = F)
# Setup
# Clear your workspace
rm(list=ls())
# Load useful packages
if (!require("tidyverse")) {
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
}
if (!require("BiocManager")) {
install.packages("BiocManager", dependencies = TRUE)
}
if (!require("EnhancedVolcano")) {
BiocManager::install("EnhancedVolcano")
library(EnhancedVolcano)
}
if (!require("gridExtra")) {
install.packages("gridExtra", dependencies = TRUE)
library(gridExtra)
}
#make a list of all the .csv files in your working directory just so you can see what's going on
X = list.files(".",".csv")
# read the raw data files
master_list <- read.csv("All_phospho_normalised_data.csv" )
# Make WT + CKO data frames
WT_data <- cbind(Phosphosite = master_list$Phosphosite,
Protein_names = master_list$Protein.names,
Gene_names = master_list$Gene.names,
master_list[,25:48])
CKO_data <- cbind(Phosphosite = master_list$Phosphosite,
Protein_names = master_list$Protein.names,
Gene_names = master_list$Gene.names,
master_list[,49:72])
# Calculate fold-changes in mean values
WT_averages <- apply(WT_data[,4:ncol(WT_data)], 1, mean)
CKO_averages <- apply(CKO_data[,4:ncol(CKO_data)], 1, mean)
fold_changes <- CKO_averages / WT_averages
log_fold_changes <- log2(fold_changes)
# Carry out Student's t-tests on each protein and output the p value, then correct using the Benjamini–Hochberg method
test_input <- cbind(master_list[,25:ncol(master_list)])
t_test_output <- apply(test_input, 1, function(x) t.test(x[1:24], x[25:48])$p.value)
BH_adjusted_p <- p.adjust(t_test_output, "BH")
# Bonf_adjusted_p <- p.adjust(t_test_output, "bonferroni")
results <- cbind(WT_data[,1:3], t_test_output, BH_adjusted_p, fold_changes, log_fold_changes)
colnames(results) <- c("Phosphosite","protein_names", "gene_names", "pVal", "BH_pVal", "fold_change", "log_fold_change")
# Export the results ordered by significance
ordered_results <- results %>% arrange(BH_pVal)
write.csv(ordered_results, "CKOvsWT_results_phospho.csv", row.names = F)
#######################################################################################################################################
# Combined analysis Notebook 4 - DESCRIPTION
#
# Here we carry out analyses to compare average phosphorylation level for each phosphopeptide, between genotypes.
# This is using data split into WT/CKO AFTER normalisation, so batch correction has occurred already.
# Data from here was not used in the publication in the end
#
# INPUTS = all normalised data (.csv)
# OUTPUTS = results of mutliple t tests with BH correction (.csv)
#
#######################################################################################################################################
# Setup
# Clear your workspace
rm(list=ls())
# Load useful packages
if (!require("tidyverse")) {
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
}
#make a list of all the .csv files in your working directory just so you can see what's going on
X = list.files(".",".csv")
# read the raw data files
master_list <- read.csv("All_phospho_normalised_data.csv" )
# Make WT + CKO data frames
WT_data <- cbind(Phosphosite = master_list$Phosphosite,
Protein_names = master_list$Protein.names,
Gene_names = master_list$Gene.names,
master_list[,25:48])
CKO_data <- cbind(Phosphosite = master_list$Phosphosite,
Protein_names = master_list$Protein.names,
Gene_names = master_list$Gene.names,
master_list[,49:72])
# Calculate fold-changes in mean values
WT_averages <- apply(WT_data[,4:ncol(WT_data)], 1, mean)
CKO_averages <- apply(CKO_data[,4:ncol(CKO_data)], 1, mean)
fold_changes <- CKO_averages / WT_averages
log_fold_changes <- log2(fold_changes)
# Carry out Student's t-tests on each protein and output the p value, then correct using the Benjamini–Hochberg method
test_input <- cbind(master_list[,25:ncol(master_list)])
t_test_output <- apply(test_input, 1, function(x) t.test(x[1:24], x[25:48])$p.value)
BH_adjusted_p <- p.adjust(t_test_output, "BH")
# Bonf_adjusted_p <- p.adjust(t_test_output, "bonferroni")
results <- cbind(WT_data[,1:3], t_test_output, BH_adjusted_p, fold_changes, log_fold_changes)
colnames(results) <- c("Phosphosite","protein_names", "gene_names", "pVal", "BH_pVal", "fold_change", "log_fold_change")
# Export the results ordered by significance
ordered_results <- results %>% arrange(BH_pVal)
write.csv(ordered_results, "CKOvsWT_results_phospho.csv", row.names = F)
# Setup
# Clear your workspace
rm(list=ls())
# Load useful packages
if (!require("tidyverse")) {
install.packages("tidyverse", dependencies = TRUE)
library(tidyverse)
}
if (!require("gridExtra")) {
install.packages("gridExtra", dependencies = TRUE)
library(gridExtra)
}
#make a list of all the .csv files in your working directory just so you can see what's going on
X = list.files(".",".csv")
# Import datasets
data_start <- read.csv("All_phospho_normalised_data.csv",stringsAsFactors = F)
WT_total <- cbind(data_start$Phosphosite, data_start$Protein, data_start$Protein.names, data_start$Gene.names, data_start[, 25:48])
CKO_total <- cbind(data_start$Phosphosite, data_start$Protein, data_start$Protein.names, data_start$Gene.names, data_start[, 49:ncol(data_start)])
colnames(WT_total)[1:4] <- c("Phosphosite", "Protein", "Protein.names", "Gene.names")
colnames(CKO_total)[1:4] <- c("Phosphosite", "Protein", "Protein.names", "Gene.names")
# Calcuate abundance of total protein datasets + take the log
WT_total$abundance <- apply(WT_total[, 5:ncol(WT_total)], 1, mean)
WT_total$logabundance <- log10(WT_total$abundance)
CKO_total$abundance <- apply(CKO_total[, 5:ncol(CKO_total)], 1, mean)
CKO_total$logabundance <- log10(CKO_total$abundance)
# Load the rhythmic proteins - only the metadata is needed
WT_hits <- read.csv("WT.p_RA_B.csv", stringsAsFactors = F)
WT_hits <- WT_hits[, 1:9]
CKO_hits <- read.csv("CKO.p_RA_B.csv", stringsAsFactors = F)
CKO_hits <- CKO_hits[, 1:9]
# Assign each protein a decile
WT_total$decile <- ntile(WT_total$logabundance, 10)
CKO_total$decile <- ntile(CKO_total$logabundance, 10)
# For each decile, calculate the proportion that are rhythmic - create function + test it
calculate_proportion <- function(total, rhythmic, x) {
a <- total[which(total$decile == x), ] # Extract the desired decile
b <- mean(a$logabundance)
c <- total[which(a$Phosphosite %in% rhythmic$Phosphosite), ] # Extract the ones in the decile which are rhythmic
d <- 100* nrow(c) / nrow(a) # calculate the proportion of the decile that appears in the rhythmic set
output <- c(b, d)
return(output)
}
calculate_proportion(WT_total, WT_hits, 10)
# Calculations for WT + CKO
rhythmic_proportions <- matrix(0, nrow = 10, ncol = 5) # Initialise empty matric
for (i in 1:10) {
rhythmic_proportions[i, 1] <- i # make the first column decile number
rhythmic_proportions[i, 2:3] <- calculate_proportion(WT_total, WT_hits, i) # Calculate for WT
rhythmic_proportions[i, 4:5] <- calculate_proportion(CKO_total, CKO_hits, i) # Calculate for CKO
}
rhythmic_proportions <- as.data.frame(rhythmic_proportions)
colnames(rhythmic_proportions) <- c("Decile",
"WT average", "WT proportion",
"CKO average", "CKO proportion")
# Export
write.csv(rhythmic_proportions, "Rhythmic proportions phospho.csv", row.names = F)
